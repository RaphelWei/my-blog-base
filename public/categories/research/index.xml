<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Research on Sihan&#39;s Blog</title>
        <link>http://localhost:1313/categories/research/</link>
        <description>Recent content in Research on Sihan&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Sihan Wei</copyright>
        <lastBuildDate>Fri, 18 Oct 2024 22:54:24 -0400</lastBuildDate><atom:link href="http://localhost:1313/categories/research/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Regret Analysis of FTRL and OMD Algorithms</title>
        <link>http://localhost:1313/p/regret-analysis-of-ftrl-and-omd-algorithms/</link>
        <pubDate>Fri, 18 Oct 2024 22:54:24 -0400</pubDate>
        
        <guid>http://localhost:1313/p/regret-analysis-of-ftrl-and-omd-algorithms/</guid>
        <description>&lt;h1 id=&#34;regret-analysis-of-ftrl-and-omd-algorithms&#34;&gt;Regret Analysis of FTRL and OMD Algorithms
&lt;/h1&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;In this note, we&amp;rsquo;ll explore the regret analysis of both the &lt;strong&gt;Follow-The-Regularized-Leader (FTRL)&lt;/strong&gt; algorithm and the &lt;strong&gt;Online Mirror Descent (OMD)&lt;/strong&gt; algorithm. We&amp;rsquo;ll highlight their similarities and differences, and demonstrate how, under certain conditions, they are essentially equivalent. This analysis includes detailed derivations and mathematical expressions.&lt;/p&gt;
&lt;h2 id=&#34;follow-the-regularized-leader-ftrl&#34;&gt;Follow-The-Regularized-Leader (FTRL)
&lt;/h2&gt;&lt;h3 id=&#34;problem-setup&#34;&gt;Problem Setup
&lt;/h3&gt;&lt;p&gt;Consider an online convex optimization problem over \( T \) rounds. At each round \( t \):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Decision Making&lt;/strong&gt;: The learner selects \( \mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^n \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss Revealing&lt;/strong&gt;: An adversary reveals a convex loss function \( f_t : \mathcal{X} \rightarrow \mathbb{R} \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss Incurred&lt;/strong&gt;: The learner incurs loss \( f_t(\mathbf{x}_t) \).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Minimize the cumulative &lt;strong&gt;regret&lt;/strong&gt;:&lt;/p&gt;
\[
\text{Regret}_T = \sum_{t=1}^T f_t(\mathbf{x}_t) - \min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T f_t(\mathbf{x}).
\]&lt;h3 id=&#34;ftrl-algorithm&#34;&gt;FTRL Algorithm
&lt;/h3&gt;&lt;p&gt;At each round \( t \), the FTRL algorithm updates the decision by solving:&lt;/p&gt;
\[
\mathbf{x}_t = \arg\min_{\mathbf{x} \in \mathcal{X}} \left\{ \eta \sum_{s=1}^{t-1} f_s(\mathbf{x}) + R(\mathbf{x}) \right\},
\]&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( \eta &gt; 0 \) is the learning rate.&lt;/li&gt;
&lt;li&gt;\( R : \mathcal{X} \rightarrow \mathbb{R} \) is a strongly convex regularization function.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;regret-analysis&#34;&gt;Regret Analysis
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Convexity&lt;/strong&gt;: Each loss function \( f_t \) is convex.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lipschitz Continuity&lt;/strong&gt;: The subgradients are bounded: \( \| \nabla f_t(\mathbf{x}) \|_* \leq G \) for all \( \mathbf{x} \in \mathcal{X} \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strong Convexity&lt;/strong&gt;: The regularizer \( R \) is \( \lambda \)-strongly convex with respect to a norm \( \| \cdot \| \).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Key Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;One-Step Regret Bound&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the convexity of \( f_t \):&lt;/p&gt;
\[
   f_t(\mathbf{x}_t) - f_t(\mathbf{x}^*) \leq \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}^* \rangle,
   \]&lt;p&gt;where \( \mathbf{x}^* = \arg\min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T f_t(\mathbf{x}) \).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Regret Decomposition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Summing over \( t \):&lt;/p&gt;
\[
   \text{Regret}_T \leq \sum_{t=1}^T \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}^* \rangle.
   \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bounding the Inner Product&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the properties of the regularizer and the FTRL updates, we can relate the sum to the Bregman divergence \( D_R \):&lt;/p&gt;
\[
   \sum_{t=1}^T \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}^* \rangle \leq \frac{R(\mathbf{x}^*) - R(\mathbf{x}_1)}{\eta}.
   \]&lt;p&gt;&lt;strong&gt;Bregman Divergence Definition&lt;/strong&gt;:&lt;/p&gt;
\[
   D_R(\mathbf{x}, \mathbf{y}) = R(\mathbf{x}) - R(\mathbf{y}) - \langle \nabla R(\mathbf{y}), \mathbf{x} - \mathbf{y} \rangle.
   \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Regret Bound&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Therefore, the total regret is bounded by:&lt;/p&gt;
\[
   \text{Regret}_T \leq \frac{R(\mathbf{x}^*) - R(\mathbf{x}_1)}{\eta}.
   \]&lt;p&gt;By choosing \( \eta \) appropriately (e.g., \( \eta = \sqrt{\dfrac{2 [R(\mathbf{x}^*) - R(\mathbf{x}_1)]}{G^2 T}} \)), we can achieve a regret bound of:&lt;/p&gt;
\[
   \text{Regret}_T \leq G \sqrt{2 [R(\mathbf{x}^*) - R(\mathbf{x}_1)] T}.
   \]&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;online-mirror-descent-omd&#34;&gt;Online Mirror Descent (OMD)
&lt;/h2&gt;&lt;h3 id=&#34;algorithm-steps&#34;&gt;Algorithm Steps
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: Choose an initial point \( \mathbf{x}_1 \in \mathcal{X} \).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;For each round \( t = 1, \dots, T \)&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;a. &lt;strong&gt;Compute Subgradient&lt;/strong&gt;:&lt;/p&gt;
\[
   \mathbf{g}_t = \nabla f_t(\mathbf{x}_t).
   \]&lt;p&gt;b. &lt;strong&gt;Dual Space Update&lt;/strong&gt;:&lt;/p&gt;
\[
   \mathbf{z}_{t+1} = \mathbf{z}_t - \eta \mathbf{g}_t,
   \]&lt;p&gt;where \( \mathbf{z}_t = \nabla \psi(\mathbf{x}_t) \).&lt;/p&gt;
&lt;p&gt;c. &lt;strong&gt;Primal Space Update&lt;/strong&gt;:&lt;/p&gt;
\[
   \mathbf{x}_{t+1} = \nabla \psi^*(\mathbf{z}_{t+1}),
   \]&lt;p&gt;with \( \psi^* \) being the convex conjugate of \( \psi \).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;regret-analysis-1&#34;&gt;Regret Analysis
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Convexity&lt;/strong&gt;: Each \( f_t \) is convex.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lipschitz Continuity&lt;/strong&gt;: Subgradients are bounded: \( \| \mathbf{g}_t \|_* \leq G \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strong Convexity&lt;/strong&gt;: The mirror map \( \psi \) is \( \lambda \)-strongly convex.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Key Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Regret Decomposition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The regret can be bounded by:&lt;/p&gt;
\[
   \text{Regret}_T \leq \sum_{t=1}^T \langle \mathbf{g}_t, \mathbf{x}_t - \mathbf{x}^* \rangle.
   \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Using Mirror Descent Updates&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Utilizing the properties of the Bregman divergence \( D_\psi \) and the mirror descent updates:&lt;/p&gt;
\[
   \sum_{t=1}^T \langle \mathbf{g}_t, \mathbf{x}_t - \mathbf{x}^* \rangle = \frac{1}{\eta} \left[ D_\psi(\mathbf{x}^*, \mathbf{x}_1) - D_\psi(\mathbf{x}^*, \mathbf{x}_{T+1}) + \sum_{t=1}^T D_\psi(\mathbf{x}_{t+1}, \mathbf{x}_t) \right].
   \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bounding the Bregman Divergences&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since \( D_\psi(\mathbf{x}^*, \mathbf{x}_{T+1}) \geq 0 \) and \( D_\psi(\mathbf{x}_{t+1}, \mathbf{x}_t) \leq \dfrac{\eta^2 G^2}{2 \lambda} \), we have:&lt;/p&gt;
\[
   \text{Regret}_T \leq \frac{D_\psi(\mathbf{x}^*, \mathbf{x}_1)}{\eta} + \frac{\eta G^2 T}{2 \lambda}.
   \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizing the Learning Rate&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Choosing:&lt;/p&gt;
\[
   \eta = \sqrt{\dfrac{2 \lambda D_\psi(\mathbf{x}^*, \mathbf{x}_1)}{G^2 T}},
   \]&lt;p&gt;yields the regret bound:&lt;/p&gt;
\[
   \text{Regret}_T \leq G \sqrt{\dfrac{2 D_\psi(\mathbf{x}^*, \mathbf{x}_1) T}{\lambda}}.
   \]&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;equivalence-of-ftrl-and-omd&#34;&gt;Equivalence of FTRL and OMD
&lt;/h2&gt;&lt;p&gt;Under certain conditions, FTRL and OMD are equivalent algorithms.&lt;/p&gt;
&lt;h3 id=&#34;conditions-for-equivalence&#34;&gt;Conditions for Equivalence
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Matching Regularizers and Mirror Maps&lt;/strong&gt;: If the regularizer \( R \) in FTRL is identical to the mirror map \( \psi \) in OMD.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unconstrained Domain&lt;/strong&gt;: When the feasible set \( \mathcal{X} \) is the entire space \( \mathbb{R}^n \).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;demonstration-of-equivalence&#34;&gt;Demonstration of Equivalence
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FTRL Update in Terms of Gradients&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The FTRL update can be expressed as:&lt;/p&gt;
\[
   \mathbf{x}_t = \arg\min_{\mathbf{x} \in \mathcal{X}} \left\{ \left\langle \eta \sum_{s=1}^{t-1} \mathbf{g}_s, \mathbf{x} \right\rangle + R(\mathbf{x}) \right\}.
   \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relation to Dual Variables in OMD&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In OMD, the dual variable \( \mathbf{z}_t \) is:&lt;/p&gt;
\[
   \mathbf{z}_t = \nabla \psi(\mathbf{x}_t) = \mathbf{z}_1 - \eta \sum_{s=1}^{t-1} \mathbf{g}_s.
   \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Primal Update via Convex Conjugate&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The FTRL update becomes:&lt;/p&gt;
\[
   \mathbf{x}_t = \nabla R^*\left( -\eta \sum_{s=1}^{t-1} \mathbf{g}_s \right),
   \]&lt;p&gt;which matches the OMD update when \( R = \psi \):&lt;/p&gt;
\[
   \mathbf{x}_t = \nabla \psi^*\left( \nabla \psi(\mathbf{x}_1) - \eta \sum_{s=1}^{t-1} \mathbf{g}_s \right).
   \]&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;By aligning the regularization function in FTRL with the mirror map in OMD and considering the unconstrained domain, the updates of both algorithms coincide. This demonstrates that FTRL and OMD are essentially equivalent under these conditions, offering different perspectives on the same optimization process.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
