[{"content":"阿Q的姓氏无从考证。他自己说，他原本在 submission system 里总是排在最后，因为 theory 的 paper 一般按字母顺序排作者名，姓赵的话，Z 是一定压轴的。他一直觉得这就是他被忽视的根源。后来有人说他姓赵，他也不否认；有人说他不配姓赵，他也不争。于是 reviewer 和 fellow student 都叫他阿Q。\n他说他是参加过 ICML 的人。虽然没有人见过他的 poster，也没有找到那年他的论文，但他很有把握地说：“我那个 paper 给了我 6 6 4，怎么就 reject 呢？你懂不懂 reviewer 是怎么想的？”\n每当有人在 group meeting 上汇报新出的 transformer 方法时，他就斜眼看一眼，冷笑道：“这不就是堆参数吗？我们那会儿讲的是 generalization error bound，这些人只是调得巧。”\n有人问：“你现在研究什么？”\n他说：“我在重写 PAC-Bayes bound，用 Rényi divergence 替 KL，这东西你们看不懂。”\n众人默然。\n他感到众人沉默，是因为被他震慑了。\n阿Q最常去的是组里的slack channel。他每天都要在群里发几段 reviewer 的 comment，一边贴，一边点评：“你看，他写了 \u0026lsquo;interesting direction\u0026rsquo;，我就知道他们 reject 我不是因为质量，是因为看不懂。”\n他的 rejection folder 里已经有七十二封信，他说这是他理论道路上的勋章。\n他常说：“我跟你们不一样，我不是为了发 paper 而发 paper，我是有思想的。那种靠 data augmentation 和 scaling law 刷榜的，是Engineering，不是 science。”\n大家起初还会应一两句，后来也就不再回复。他便更加坚定地认为，自己已经远远走在了他们前面，只是这些人还没有意识到而已。\n有一天，他偷偷在 arXiv 上挂了一篇和 LLM 有关的短文，标题是《Towards Understanding Prompt Optimization via PAC-Bayesian Lens》。\n有人在群里贴了一条招聘信息：“某大厂招 NLP research scientist，要求熟悉 prompt tuning。”\n他立刻发言：“现在的大厂啊，早就不搞 serious research 了。真正想做 science 的，是不会去做 prompt engineering 的。”\n同学问：“你那篇 PAC-Bayes 的 prompt paper，是不是也在调 prompt？”\n他一愣，旋即大笑：“我那是从原理上解释 prompt 的 generalization 啊，能一样么？懂点 math 再来问我。”\n他的 advisor 在slack群里祝贺 lab 里有几个同学发了 NeurIPS、ICLR，还有一个拿了 best paper。他没有恭喜，也没有发祝贺表情，而是悄悄发了一条朋友圈：\n“人心浮躁，真正的研究者不屑争名逐利。愿守住理论的孤独。”\n底下没人点赞。\n他觉得大家是在默默支持他，怕显得太明显。\n那天他又收到了一个 rejection。\n他把 OpenReview 上的 rebuttal 页面截图发到了群里，配文：“胜利了！胜利了！Reviewer B 开始认真提建议了，还说我的 proof 有启发性，这说明我的方向引起了重视。”\n他的 rejection folder 里多了一封信，他的精神胜利记录，也就更完整了一点。\n后来有人发现，他那年所谓的 ICML paper，是和赵教授的组联合投稿的，最终被 withdraw 了。\n赵教授是著名的 last author，一直稳居 alphabet 排名的末位，却从不忘在所有论文里标注自己为 corresponding。他说：“阿Q？只是来旁听过几次组会，连共一作都不算。”\n阿Q愤愤地说：“那不过是借了我一点想法罢了。”\n然后他又补了一句：“我其实……也算是赵家人。”\n没人应声。\n他笑了笑，自顾自地关了 rejection 页面，打开 OverLeaf，开始写他的新草稿。\n标题是：《A Philosophical View on Generalization in the Age of Data-Centric AI》。\n他觉得，这一次，他们总该看懂了。\n(未完待续)\n我又不知道阿Q的名字是怎麼寫的。他活著的時候，人都叫他阿Quei，死了以後，便沒有一個人再叫阿Quei了，那裏還會有“著之竹帛”的事。若論“著之竹帛”，這篇文章要算第一次，所以先遇著了這第一個難關。我曾仔細想：阿Quei，阿桂還是阿貴呢？倘使他號月亭，或者在八月間做過生日，那一定是阿桂了；而他既沒有號——也許有號，只是沒有人知道他，——又未嘗散過生日徵文的帖子：寫作阿桂，是武斷的。又倘使他有一位老兄或令弟叫阿富，那一定是阿貴了；而他又只是一個人：寫作阿貴，也沒有佐證的。其餘音Quei的偏僻字樣，更加湊不上了。先前，我也曾問過趙太爺的兒子茂才先生，誰料博雅如此公，竟也茫然，但據結論說，是因為陳獨秀辦了《新青年》提倡洋字，所以國粹淪亡，無可查考了。我的最後的手段，只有托一個同鄉去查阿Q犯事的案卷，八個月之後纔有回信，說案卷裏並無與阿Quei的聲音相近的人。我雖不知道是真沒有，還是沒有查，然而也再沒有別的方法了。生怕註音字母還未通行，只好用了“洋字”，照英國流行的拼法寫他為阿Quei，略作阿Q。這近於盲從《新青年》，自己也很抱歉，但茂才公尚且不知，我還有什麼好辦法呢。\n― 鲁迅, 《阿Q正传》 ","date":"2025-03-22T19:22:18-04:00","image":"http://localhost:1313/zh-cn/p/%E9%98%BFq%E6%AD%A3%E4%BC%A0%E4%B8%80/aq_hu10122460770916415340.jpg","permalink":"http://localhost:1313/zh-cn/p/%E9%98%BFq%E6%AD%A3%E4%BC%A0%E4%B8%80/","title":"阿Q正传（一）"},{"content":"引子：致敬鲁迅先生 公司 pantry 的角落里有一块白板，常年写着一些谁也看不懂的公式。旁边一张桌子，散着几张顶会的 reject letter，纸上 reviewer 评论说“bound is tight but practically irrelevant”。这地方原来是留给 visiting intern 用的，现在久而久之，就成了孔乙己的“工位”。\n孔乙己是做 learning theory 的，据说读过书，还在 NeurIPS 上发过 poster。有一次 all-hands meeting，他在群聊里发了一串 LaTeX 数学公式，说是要解释 AUC 和 generalization gap 的关系，结果被 HR 警告了一次，说“请文明发言”。\n他每天中午都会在咖啡机旁遇到我们这些实习生，常常问：“你读过 learning theory 吗？”我们一开始还会点点头，后来知道他下一句总是：“我便考你一考。generalization bound 是怎么写的？”便学乖了，低头冲咖啡，假装没听见。\n有一回，他对我说：“你知道 uniform convergence 吗？……我教给你，记着！这些 bound 应该记着。将来做 senior scientist 的时候，写 promotion doc 要用。”我心里好笑，我们组连 offline metric 都快不要了，还 promotion doc 呢。\n我懒懒地答他道：“谁要你教，不就是 Hoeffding inequality 加个 union bound 吗？”\n孔乙己显得极高兴，将两根细长的手指在白板上敲着，点头说：“对呀对呀！……PAC bound 有四样写法，你知道么？”\n我努着嘴走远。他刚用咖啡搅拌棒蘸了点拿铁，想在白板上写式子，讲一讲 Rademacher complexity 和 KL divergence 的 tradeoff，见我毫不热心，便又叹了一口气，显出极惋惜的样子。\n他其实也不是不想工作，只是进了组以后一直想做一个“理论支撑部署”的项目，写了半年 proposal，最后老板让他先 review 一下现有的 pipeline。他不服，写了一封一万五千字的 reply，附上五页 appendix，解释“为什么这不是我应该做的工作”。\n现在大家说他是“last theory holdout”，还没接受 large language model 统治世界的事实。有人问他，“你怎么看 foundation models 的 future？”他一边抿着剩下的咖啡，一边念叨：“large model…… large variance…… no bound……怎么能 deploy 呢……”\n他是真的读过书。只是这年头，读过书的人没人 care bound，写不出 report summary 的人，连实习都留不下来。\n第一章：读过书的人 孔乙己是做 learning theory 的。\n最早知道他，是在 Reddit 上的一个 AMA。他还在读 PhD，头像是一张顶会 poster，署名第一，学校是个大家都听过的藤校。他的自我介绍这样写着：“做 generalization 理论，探索真实世界学习背后的本质规律。”\n在那个帖子底下，他写了长长的一段话，引用了 PAC learning、VC dimension 和 Occam\u0026rsquo;s Razor，又狠狠批了 transformer：“你们这些搞 LLM 的，连 basic bound 都不会推，还敢说是做科研？”底下有人反驳，他立刻贴了十几条 arXiv 链接，说自己不屑于和“只会调超参的人”辩论。\n那年，他刚上四年级，刚好 paper 被 NeurIPS workshop 收了个 oral。他在朋友圈发了一张自己在黑板前讲 bound 的照片，配文是：“This is what real understanding looks like.”\n导师是圈内出了名的学院派老教授，paper 写得比代码还多。他曾在系里 seminar 上说过：“做 theory 的人，不能和 trend 同流合污。”\n孔乙己听了频频点头，讲座结束后站起来问了三个问题，都是关于“能不能给 attention 的 generalization bound 更 tight 一点”。他自己其实没有看 transformer，但这不妨碍他用七页证明说明“这种结构 inherently 不 generalize”。\n大家都说，这人——读过书。\n第二章：找实习的人 孔乙己是读过书的，可是，找不到实习。\n他开始投实习那年，ChatGPT 正好火遍全球。校内的同学不是在 OpenAI 实习，就是在 Anthropic、Google DeepMind 做 LLM pretraining。大家都在做 self-supervised learning 和 alignment，他却仍然执着于自己那套“distribution shift 下的 uniform stability”。\n导师让他写简历。他花了两周，写了六页 PDF，全是 theorem 和 lemma，标题叫：“Research Summary – Towards Trustworthy Learning via Rigorous Risk Control”。最后一页是他自己画的图，图注写着：“Figure 4.7: Bound does not improve, but it is tight。”\n没有一段工业经验，也没有代码链接。他曾经尝试加一个 GitHub，但主页上写着：“Repository removed due to theoretical errors in implementation assumptions.”\n他照常投了三十多家公司，几乎都石沉大海。\n他后来听说，一个跟他同届的学生，主攻 AutoML，发了一堆调参论文，进了 Meta 的 RLHF team。他不服，说：“这种连问题都没搞清楚的领域，发 paper 如流水，根本谈不上科学。”他在自己的 note 里写道：\n“真理不可通过 A/B test 验证，理论才是机器学习的灵魂。”\n他还申请了一个 MLE 实习，面试官问：“你怎么看最近 dropout 在 production 中的不稳定性？”\n孔乙己答：“你们这是 overfitting 实践经验。我有一篇未发的论文，指出 dropout 实际上可以用 PAC-Bayes 框架来统一解释。”\n面试官礼貌微笑，说：“好深奥。我们 infra 组暂时还用不到。”\n那年夏天，他没有找到实习。他仍然每天在 lab 里推式子，有时去旁听 LLM 组的组会，听一半摇头离开。他说那些人“连 math 都不配谈”，用的词全是“token”、“prompt”、“alignment”，没有一个是“loss”、“bound”、“hypothesis class”。\n他其实也羡慕，但他嘴硬。他说：“他们是潮水，我是地基。等这股风过去了，大家会想起我的。”\n只不过那股风，过去了一个 transformer，又来了一个 SSM。他依旧在白板前写 KL divergence，写着写着，就又到了秋天。\n第三章：毕业的人 孔乙己是在第七年毕业的。\n原本他第五年就打算毕业。那时候他还写了一份邮件草稿，要群发告别，说自己将“以理论之眼投身现实”，实现“从 PAC 到 Product 的跃迁”。但那年，他把最得意的一篇论文投到了 ICML，被 Reviewer 2 留下一句评价：\n“The bound is tight but practically useless.”\n他一怒之下没有 rebuttal，把论文重新写了一版，把所有式子换成了 measure-theoretic 的表述，结果篇幅超了三十页，NeurIPS desk reject。\n第六年，导师说：“你可以考虑毕业了。”\n孔乙己说：“我还没写完那篇 information-theoretic regret bound 的 follow-up 呢。再给我一年。”\n导师点点头。他已经习惯了孔乙己推迟进度，也知道自己早已无法干预这位弟子对 tight bound 的执念。\n这一年，实验室搬了新楼，年轻的师弟师妹都用起了 JAX 和 PyTorch 2。他仍然在 server 上 ssh 进一台 Ubuntu 14.04，写着纯 numpy 的脚本，运行一个只能复现 appendix 图的 toy example。\n第七年春天，他终于发出了一篇论文，是个小 workshop，在一个很新的理论 track。他在推特上发了动态：\n“七年磨一式，唯愿后人得见真实的 complexity。”\n没人点赞。\n他终于决定毕业。简历依旧是六页，PDF 里还加了页码，脚注引用了自己两篇 rejected 的 arXiv。导师给他写了推荐信，语气也比从前谨慎了些，只说他“学术兴趣明确，逻辑严密，独立性强”。最后一句是：“更适合研究型岗位。”\n后来他去了一个 B 轮创业公司做 research engineer。公司在做“可解释性 + differential privacy”，是个 niche niche 的领域。\n他的 offer 上写的是：“title：Research Engineer；scope：参与理论模型验证和文档撰写；base：比市场价略低。”\n他没讲价。他说自己“只求一个地方继续思考”。\n第四章：进公司的人 孔乙己进了公司，却从未真正融入过公司。\n他的办公桌在走廊尽头，旁边是一台没人用的打印机和一面已经褪色的公告栏。他搬来那块写了七年的白板，用酒精擦了一遍，上面仍隐约可见 “\\(\\mathbb{E}_{S \\sim D^n}\\left[R(h_S)\\right] \\leq \\hat{R}_S(h_S) + \\text{Complexity Term}\\)” 的痕迹。\n他每天早上八点半就来，泡一壶冻顶乌龙，打开 terminal，进一个 bound_experiments 的目录，里面是他复现自己 PAC-Bayes 推导的脚本。每次模型跑出来结果，他不是看 accuracy，而是把 empirical risk 和 KL divergence 画在一张图上，用 marker 在白板上标注：“偏差可控。”\n他其实是有项目的，公司的产品在做 privacy-preserving recommender，他的任务是“从理论上分析机制稳定性”。但大家都在忙上线和 metric，他交了两份报告，没人看；他约了一次 1-on-1，老板说：“你这个方向我们暂时搁一搁哈，先看看 infra 那边的优化能不能搞搞。”\n他点头，说“可以”，回工位后继续看那篇发表于二十年前的论文：《On the Stability and Generalization of Learning Algorithms》。\n有一次组会，大家在讨论一个新上线的模型效果，PM 问：“有没有人能解释一下为什么线上 performance 比 offline 高？”\n孔乙己清了清嗓子，说：“其实这是 non-IID 训练下的一种局部 regularization 效果，我最近写了一个 note，里面讲了 generalized Rademacher complexity 在局部区域的演化过程，可以 unified explain 这类现象。”\nPM 点点头，说：“你这个听起来好厉害……但能不能先看看线上用户路径是不是变了？”\n他开始变得沉默。\n同一批进公司的同事，有的成了 tech lead，有的跳槽去了大厂做 foundation model 预训练。他还在看那些没人看的 arXiv 论文，改着自己的旧代码。有时他也试图转向热门方向，比如看了看 alignment safety，想写个“bound-aware preference model”，写了三页之后发现根本找不到数据集，就搁下了。\n他拒绝了写文档，也不喜欢开 sprint。他说：“我不是来写 JIRA ticket 的。”有同事劝他：“你可以转 Applied 吗？可能会更有 impact。”他摇头：“做理论的，不能退。”\n后来公司裁员。他没有被裁，但老板和他说：“要不你去 infra 帮忙？最近要重写 feature store，缺人。”\n他点点头，说：“好。”但第二天，他照旧坐在角落，把白板擦干净，重新写下那一句：\n“A tight bound is still a bound.”\n他开始变得透明。新人不知道他是干什么的，以为他是 legacy team 留下的“技术文献管理员”。只有年会上有人还记得，举着啤酒说：“欸，你就是那个做 PAC 的吧？”\n他笑笑，举杯，说：“做过。”\n第五章：变成传说的人 没人记得孔乙己是什么时候开始变得“神秘”的。\n一开始，他还时常在 Slack 上发点理论笔记，转发一两篇 ICML 的 old-school 论文，底下偶尔会有人点个赞，更多的时候是既看不懂也不回复。后来他发得少了，只在公司的 Confluence 上写长文，标题常常是：\n“On the Unstated Assumptions of Generalization in Real-World Deployment”\n“A Note on the Stability of Feature Drift under Distribution Shift”\n“Appendix: Why ‘Test Accuracy’ Is Not the Whole Story”\n这些文章没人评论，也没有 tag reviewer。他自己默默上传、默默保存，有时会在文章末尾留一句话：\n“I don’t expect anyone to read this now. But one day, someone might need it.”\n再后来，他的头像变灰了，Slack 状态永远显示“Do Not Disturb”，组会也不再出声，只是静静地坐着，偶尔点点头。\n有实习生新加入，看到他办公桌上那块白板，上面写着一行公式：“\\(\\mathrm{KL}(Q \\| P)+\\widehat{R}_S(h) \\leq \\mathcal{B}(n, \\delta)\\)”。他们小声议论：\n“这是谁的座位？” “听说以前是个做 theory 的。” “还在公司吗？” “不确定，org chart 上已经找不到了。” 有一次 infra 组线上开会，发现某段老代码竟然有注释，写得极细致，还附带了 markdown 文档和理论推导。大家惊讶地说：“现在谁还写这么认真的文档？”\n有人点开 commit history，署名是：KongYJ@company.com\n大家恍然大悟：“啊，是他。”\n但再往下找，他已经半年没提交过任何代码。Git 账号也处于 dormant 状态。只有每个月底的 meeting notes 里，还会悄悄出现一两条评论，引用某篇十年前的 paper，说：“可以从 complexity 的角度再看看这个问题。”\n没人回他。他也不再等待回复。\n每年年末，公司有一个“年度最佳文档”评选，鼓励员工撰写高质量的技术总结。那年不知谁提名了孔乙己的一篇旧笔记，标题叫：\n“What Is Lost When We Forget the Bound?”\n没有人投票，但 HR 给了他一个“特别致敬奖”，附赠一张印着公司 logo 的笔记本。他收下了，说了一句：\n“谢谢。我会继续记着这些。”\n至此，他已然不属于任何 team。也没人知道他工号还是否有效。他的电脑仍然每天按时上线，Slack 每周固定显示 green light 两小时，Confluence 上每隔一个月更新一篇无访客的新笔记。\n偶尔有人路过角落的那张桌子，看见那块白板上仍写着那句旧话：\n“Bound is tight. Heart is broken.”\n终章：不在 chart 上的人 那年，公司转型做 LLM 了。\n老板在 All Hands 上激动地宣布：“我们下一阶段的战略重点，是构建企业级大模型平台。我们要对标 OpenAI！”全场鼓掌，连 infra 都有人开始上 huggingface。\n技术文档全换成 prompt 设计、LoRA 微调、in-context learning。组会的名字也改了，叫做“Alignment Brainstorming Sync”。每周有一半时间用来试不同 checkpoint，剩下的时间就是写 prompt 和总结 slide。\n孔乙己当然是不适应的。他翻了翻那篇爆红的 LLM 论文，低声念了一句：“连定义都不写清楚，也能叫 paper？”\n他试图参与一次 prompt 讨论会，说自己在想“bound-aware prompt optimization”，想设计一个目标函数，把 prompt tuning 变成一个带分布不确定性的 optimization 问题。\n没人懂他在说什么。有个组员好心劝他：“我们现在走的是 empirical way，不太考虑 generalization theory 啦。”他说：“你们这是回到 pre-Vapnik 时代了。”\n他渐渐不再参加组会。部门 chart 更新后，他的名字没有出现在任何一个 team 下。有人问 HR：“孔乙己是不是离职了？”HR 查了一下，说：“不是，他的工号还在。但没有 active assignment。”\n再后来，公司换了楼。他的工位被清掉，白板也被擦干净，只留下几条模糊的笔迹，像极了一些无法证伪的公式。\n白板边缘贴着的一张纸掉落在地，上面写着：\n“如果世界都不再在意 bound，那我就是最后一个 bound 的守夜人。”\n最后一次有人“见到”他，是在某个开源 repo 的 commit 记录里。\n那天，有个新入职的 engineer 遇到一个稀奇古怪的 loss function，顺手在公司旧文档里搜了一下，竟然跳出一篇三年前的笔记，作者正是孔乙己。他试着按照那篇笔记的推导重写了代码，metrics 提高了 0.3%。他在 repo 上开了一个 PR，title 写着：\n“Implement forgotten bound (credit to K.Y.J.)”\n第二天清晨，PR 下多了一条评论：\n“Glad to know someone still cares about the bound. – K.”\n再之后，账号不再上线。\n不知过了多久后的一天，某个新来的实习生，在一个过时的 dashboard 页面底部看到这样一行小字：\n“PAC bounds are not dead. They are just waiting. — K.Y.J.”\n他问：“这是谁？”\n没人回答。\n但某个老员工听见这句话，笑了。他说：“孔乙己还欠十九页报告呢！”\n到了春天，他又说了一次。再往后，再也没人提起过他。\n","date":"2025-03-22T04:17:24-04:00","image":"http://localhost:1313/zh-cn/p/%E5%AD%94%E4%B9%99%E5%B7%B1/kongyiji_hu7718234728128522531.jpeg","permalink":"http://localhost:1313/zh-cn/p/%E5%AD%94%E4%B9%99%E5%B7%B1/","title":"孔乙己"},{"content":"本系列文章由我与 ChatGPT 共同创作。\n名字取自鲁迅先生的《故事新编》。先生尝言：“只取一点因由，随意点染。”我们也效法其意，取诸子百家之形，借机器学习之壳，以文学笔法写学术众生，以讽刺调侃慰理论人心。若能博君一笑，甚是欣悦。\n本系列始于“ML鲁迅宇宙”，因情而发，源于一次读 paper 发疯的夜晚。彼时忽觉自己与那位“脱不下长衫”的孔乙己何其相似，遂仿效先生旧作，略作改写，权作一名 theory 学生的自言自语。\n此为《故事新编》之序。\n","date":"2025-03-21T18:53:08-04:00","permalink":"http://localhost:1313/zh-cn/p/%E5%BA%8F-%E6%95%85%E4%BA%8B%E6%96%B0%E7%BC%96/","title":"序 · 故事新编"},{"content":"引言 2021年就快要过去了，我很怀念它。\n其实我本来是不太爱写年度总结这种东西的，一方面是因为懒，另一方面也是因为懒。\n不过今年总归是有点特殊的，毕竟经历了不少人和事。从旧的学校毕业了，在新的学校开始了新的旅程。于是最后决定写点什么纪念一下。\n说是年度总结，其实更像是个申请总结。时间跨度大概是从去年暑假到今年开学。\n八月 我的Thesis，或者说Capstone Project更合理一点，是实现Nesterov那篇关于Tensor Methods的算法。简单来说，常用的gradient descent是基于泰勒的一阶展开\n$$ f(y)\\approx f(x)+\\langle \\nabla f(x),y-x\\rangle $$然而大家都知道first-order method收敛很慢，即使是在strongly convex的条件下也只有 \\(O\\left(\\log(1/\\epsilon)\\right)\\)。Nesterov提出了一种利用 \\(p\\) 阶\\((p\\geq2)\\)泰勒展开来求解优化问题的算法\n$$ \\arg\\min_{y\\in\\mathbb{R}^n}\\sum_{r=0}^{p} \\frac{1}{r !} \\nabla^{r} f(x) [y-x, \\ldots, y-x]+\\frac{M_p}{(p+1) !}\\|y-x\\|_{2}^{p+1} $$这个算法的优点是快，在条件拉满的情况下可以达到\n$$O\\left((1/\\epsilon)^\\frac{M_p}{5p+2}\\right)$$这个算法的缺点是，条件过于严格，又要convex又要Lipschitz。没人会傻乎乎地用这玩意训练神经网络。这段划掉。\n这个东西写了大概两个月，暑假结束的时候跟老板简单聊了聊。老板建议我把代码整理一下，可以搞个package。然后设计一些实验，看看能不能投出去。\n我最后拒绝了这个提议，因为我觉得这玩意实在没啥用。\n八月底的时候正式把申请提上了议程。我简单地做了个个人主页，优点是发套瓷邮件的时候不需要在附件里放CV，直接在邮件正文里附上主页链接就好了。因为很多老师不太喜欢下载附件。另一个好处是可以在网页里加上tracker，这样在Google Analytics里根据主页访问记录的地址和时间就可以判断哪些老师浏览了我的主页，从而继续跟进。\n因为疫情，当时好多学校都hiring freeze了（虽然这个是针对AP招聘的，不过侧面也反映出学校没钱）。当时心里很慌，因为感觉疫情加持下的申请会异常激烈。有一天逛一亩三分地的时候，看到一位Georgia Tech的AP在找RA，而且他本人还是UMN的校友，就很高兴地给他发了封邮件。他很快就回复了，说是约个时间面试。\n面试很流水线化，我简单做了个slides讲了一下就结束了。这个老师之前是ME系的PhD，现在Aerospace Engineering (就是搞火箭的那个系)当AP。之前他一直在做控制理论和优化，不知道为什么突然转行搞起ML theory了。他想让我做nonconvex-nonconcave minimax optimization，相当于是给我挖了个新坑。因为GAN和adversarial training很火，催生了这么个研究它们背后理论的领域。我想了一下，好像可以从optimizer的角度搞点东西，现在主流的训练还在用Adam或者SGD，说不定可以把我之前做的破烂往里面塞一塞。我最后搞了个second-order(再高阶的用来训练神经网络想着都玄幻)的方法，在MNIST跑了下效果还行，不过ImageNet还是算了。这个坑填到这我就放弃了，别问，问就是拿到offer了，谁还给他免费打工啊。\n九月 新学期开学的时候，系里面组织了一个类似于新老生见面会的活动。新生会填一个问卷，回答诸如research interest，以及对哪个lab或者professor感兴趣的问题。系里面会统一收集起来，根据新生的兴趣给他们匹配lab。然后每一个lab出一个老生作为代表 (Lab Ambassador)，回答新生的问题。\n我当时为了老板能在推荐信里给我多写几句好话，便主动请缨。完事之后，系里面还让我们把Twitter个人主页好好弄一下，然后带个UMN CS的tag发推自我介绍，这样新老生相互之间可以很快认识（不得不说美国人真的很热衷于connection这一套）。我闲得无聊把自己几百年不用的Twitter也找出来稍微搞了一下，没想到这个后来居然给我带来了不少机会，下面会谈到。\n这学期还申请到了TA，不过是在商学院，给一群Business Analytics专业的学生教Python。当TA的经历也蛮有意思的，给这些学生讲课让我想起了自己当年刚学写代码的时候也是啥都不懂，总是问一堆很天真的问题，虽然现在代码写的依然很烂。\n第一次Office hour的时候，一个黑人小哥发邮件问我，为什么按照课上示例的代码运行会报错。我看了下他发的截图，检查之后发现代码没有问题，就给他开了个Zoom视频，发现他用Spyder（就是下载anaconda自带的那玩意儿）一行一行地在运行代码。于是运行到if语句那一行的时候，编译器说你这个代码不完整啊，就给报错了。我就给他讲，你点这个绿三角运行整个文件就好了。小哥对我很是感激，然后就盯上我了，可劲给我发邮件问问题，有一次直接说\nI\u0026rsquo;m writing to ask if we can meet physically\u0026hellip;\n直接给我整害怕了。那封邮件我没回，后面这小哥好像就自暴自弃了，好几次作业都没做。\n这门课还有一个叫Sarah的美国女生，我对她印象很深，因为她每次Office hour都会问一个多小时的问题，经常在后面排队的学生因为等太久最后全溜了。然后她每次结束的时候都会给我说好多遍sorry和thank you，然后下次继续。可惜学期过去三分之二的时候她withdraw了，当时我还有点小泄气，感觉之前的努力都白费了。\n十月 上面提到我把自己的Twitter主页搞了一下，在Bio里加了一句\nI am looking for a PhD position from Fall 2021.\n接下来除了写套瓷邮件之外，还会在Twitter上follow一些我关注的老师，绝大多数都是Junior Professor，因为我觉得这些人最缺人，成功率比较高。\n意外的是，我刚给一个老师点了关注，他就直接回关了，还给我发了一封邮件\n这个老师做的是distributed optimization。我跟他前前后后大概meeting了四次左右，我觉得他人非常Nice，学术水平也很高，但是本身他是控制背景出身，发的很多都是那边的会议，比如CDC，ACC什么的。我自己可能还是更想做传统的ML一点。于是最后没有申请他家。 除此之外，我还套瓷了一位老板的熟人，他也很快回我了，说最晚会在一月底面试，还说他跟我老板很熟，问我老板会不会给我写推荐信。\n我本来想着靠老板这层关系怎么着也能捞个面试，结果就被放鸽子了。\n十一月 这个月还是比较辛苦的。写SOP，上课，写作业，final project，TA的工作也不能落下，每周还要开组会。总之每天都是焦头烂额的状态。\n感恩节假期窝在家里把SOP肝了出来，让几位好朋友帮忙看看，提了提意见。不放心又花钱去一个叫papersogay的网站上找母语者改了一下。这个网站名字虽然听起来很gay，但实际使用起来一点也不gay，推荐的mentor里有很多好看的小姐姐。我找了一位Yale本科毕业的白人小姐姐，价格是按字数收费，花了100多刀吧，最后成稿还算满意。\n在填网申之前最后找老板聊了一次。老板问我申请了多少项目，我说10个左右。老板说你申请太少了，今年这情况15个才保险。于是我又赶紧补申了几个。\n针对我的SOP老板还提了一嘴，说我写得不错，但是research interest写得太窄了，全篇都在讲你对theory感兴趣。做theory的老师就那么几个，还不一定今年招人。有些做应用的老师可能对你也感兴趣，但你SOP这么写，人家可能就觉得你不想搞application，你就会因此失去很多机会。\n老板还提到，当年他申请的时候，虽然已经有7-8篇不错的文章了（不错，指CVPR一作），但是还是因为SOP写得太窄而丧失了一些机会。当时他拿了不少不错的offer，比如Princeton，UIUC，UW之类的。CMU还给他发了conditional offer，因为他托福没考到100。结果他又考了2次，全是99，就放弃了CMU。我表示难以置信，因为他在新加坡待了很多年，不像是会被英语拖累的人。\n最后我问老板今年打不打算招人，他说还没想好，不过就算招人也最多收一个，因为组里人实在太多了。他才来我们系第二年，已经7个PhD了(绝大多数是从别的老师那里跑路过来投奔他的)，还有一群MS和本科生，有点带不过来。\n有意思的是今年我们组里一共三个人申请PhD，老板最后一个人也没要，而是招了个Native Hawaiian。主要是之前我们组里只有中国人和印度人，看来老板为了diversity也是费了不少心。当然最后大家的去处都不错，另外两个印度妹子分别去了Wisconsin和UCSB。\n十二月 这个月主要就是填网申。绝大多数学校的deadline都是12.15，老板让我开个Google共享文档，他提交一封推荐信就打个勾，我也好实时跟进状态。\n然而事实证明根本就没要，老板效率实在一流，我的邮箱一直在提示收到邮件\nProfessor XXX has submitted a recommendation letter for you\u0026hellip;\n大概半小时左右，17个项目的推荐信就全部提交完毕了。\n一月 一月份的记忆有些模糊了，毕竟过去了太久。那段时间刚刚提交完申请，心里面多多少少有些忐忑。正好寒假闲在家里无事，就想着要不要再套套瓷。本着死马当成活马医的心态，索性又给自己感兴趣的AP写了好几封邮件。结果不出所料，果然绝大多数邮件都石沉大海。\n然而还是有一位PSU的老师捞了我一把。她说她面我主要是因为我是CS的学生，却想申EE的PhD，因为在她印象里CS的学生很好找工作，去湾区随便就是十几万刀。我没好意思告诉她我就是因为代码写的烂找不到工作才申请PhD的。\n聊了一会后她说她做的领域还是比较辛苦的，因为你可能浪费几个月时间就为了搞懂一段证明，最后还一无所获。她说我给你一篇我之前的论文，你读一下，做个presentation，我们再谈你要不要继续做这个方向。于是约了2月10号再meeting一次。\n二月 8号的时候收到了Boston University的PhD录取，我记得当时我正在睡午觉，被邮件声吵醒的时候还有些恼火，然而看到开头的Congratulations果然还是没绷住笑了出来。\n说实话我跟BU其实还挺有缘分的。18年申master的时候，当时心里没底，看BU写的是2.15截止，想着病急乱投医不申白不申（90刀申请费⚠️），干脆也填了网申。然而那会他家拒我特干脆，1.22提交的网申，1月底就给我发来拒信了。说实话当时心里没啥波动，安慰自己说overqualified了，唯一肉痛的就是那90刀的申请费。\nBU给的钱多，PSU钱少；BU在大城市Boston，PSU在村里。没有犹豫，我果断写邮件给之前PSU的那个老师，说我拿到了offer了，paper也不读了，presentation也不做了，直接开摆。\n二月中的时候面了CU Boulder。其实他家CS我没啥特别想跟的老师，不过因为通神在那念书，想着不行的话去那跟通神做个伴也不错。 面我的是个国人AP，因为SOP里提了他的名字所以倒也在意料之中。上来开始跟我整多普勒效应，直接人麻掉了。完事还顺带跟我聊了一下线性代数，好在那段时间一直在搞Optimization，很多基本概念也都比较熟悉，因此侥幸过关。\n聊到最后他开始给我画饼，说Boulder是个好地方，来了可以滑雪，比你申请的什么UIUC啊，Purdue啊地方好多了。还说你来了第一学期应该是做TA，但是系里面应该会额外发个几千刀的小奖。我当时一听这话以为自己稳了，然而三月初收到了拒信。\n二月底的时候收到了现在advisor的面试邀请。一开始面我的是组里的大师兄，简单聊聊做过的项目，考察了一些基本知识，愉快结束。过了几天二师兄又发来面试邀请，同样的流程，讲讲project，二师兄还问了我最喜欢的ML算法是什么，我说是SVM，毕竟让我第一次接触到了Lagrange dual，从此走上了搞Optimization的不归路。过了一周老板也发来面试邀请，好家伙，搁这升级打怪呢。\n跟老板的面试其实自我感觉表现挺一般的，老板一直在抠细节，抓住一个问题咬着不放，一路问下去，人直接傻掉。好在最后老板看出了我废物的本质，跟我介绍了一下组里的情况后就草草结束了视频通话。\n三月 三月开门红，没错，因为拒信的颜色是红的。CU Boulder在我自信满满的时候一招出其不意成功地让我破防了。\n8号是BU的Open House。因为疫情没法去现场，可惜，本来还想公费去Boston玩一趟。\n后面的日子就是在焦虑中度过的，一天刷800次一亩三分地和gradcafe，听到Gmail的邮件提示音就要赶紧打开看看是不是收到offer或者拒信了。然而不仅不是offer，在绝大多数时候连拒信都不是。\n眼看着地里面的PhD汇报帖越来越少，我不得不接受一个沉痛的事实——我80%的申请应该都沉到鱼塘里了。于是我果断给我现在的老板发了一封邮件，问他你到底还收不收我，给个痛快话。\n我记得特别清楚，那天是周一。周二就收到了JHU的集体AOE系统据。这件事他戏剧性就戏剧性在这里，因为实在太巧合了，以至于我以为这封拒信是自己催出来的。\n节目效果发生在周三，我老板回我邮件了————他说他会催committee尽快做决定。\n我心想你这不是跟我俩搁这扯犊子了吗，我都收到拒信了，还要尽快做啥决定？我就回他，我已经收到拒信了。结果他说，不可能，肯定是系统搞错了，他还没做任何决定呢，让我再等着。\n于是第二天我收到了系里小蜜发来的撤回拒信的邮件。\n得，继续等着吧。\n四月 16号，踩着线，拿到了JHU的offer。果断接了。开香槟咯！\n五月 该吃吃，该喝喝，准备准备就从UMN毕业了。因为疫情原因今年也是线上的毕业典礼，然而我本该去年就毕业的，因此内心毫无波动。\n六月 欧洲杯开赛了，每天就窝在家里看球。室友哥伦比亚小哥在家里看美洲杯和世界杯预选赛，我看欧洲杯，两个人聊得不亦乐乎。每天就是嗯混，整天醉生梦死，不知今夕是何年。\n月底的时候，老板终于联系我了，说要跟我聊聊下学期的选课以及科研计划。老板给我包办了三门课，然后告诉我，让我做adversarial robustness。接着就让我继续享受假期了。\n七月 1号的飞机，Minneapolis飞Denver，开始准备享受假期。\n很惭愧，自从18年来了明尼之后，我就再也没坐过飞机，这次算是把飞机坐爽了。\n到了Denver之后，由通神担任免费司机把我接回他家。这次Boulder之旅印象最深的就是去Rocky Mountain National Park玩了一趟，拍了不少照片。\nEstes Park 家附近的公园 可爱的小花栗鼠 好山好水好风光 双人成行 博尔特 德州牛排嗯造 这次旅行的感受就是，Boulder真是个好地方，有车真好。 在Boulder玩了半个月，回来去校医院做了个深度洗牙。躺在病床上，听牙医用小钩子在我的牙齿表面刮来刮去，咯吱咯吱，问题是自己啥也看不到，简直让人毛骨悚然。因为打了麻药，半边脸感觉肿的像猪头，吃东西像在吃棉花糖，吸管咬在嘴里像是软的。\n月底的时候就飞来Baltimore了。在明尼呆了三年，走的时候也没有太多舍不得，只是后悔还有好多地方没去过，学校附近还有好多店没吃过，还没跟老板好好道个别。\nFarewell, Minneapolis. Hello, Baltimore.\nGoodbye, Gophers. Hi, Blue Jays.\n总结 三年羁旅客\u0026hellip;\n脑子里不知怎么就想起这句诗。在明尼的三年，认识了很多有趣的人，也经历了很多刻骨铭心的事。接下来的一年，希望能好好上课，好好做科研，好好锻炼身体。最大的希望是能回国一趟，见见那些好久没见，想见却又不得见的人。毕竟，\n我亦飘零久。\n","date":"2021-12-29T18:37:42-05:00","image":"http://localhost:1313/zh-cn/p/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/lake_hu984971498550085312.JPG","permalink":"http://localhost:1313/zh-cn/p/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/","title":"2021年度总结"}]