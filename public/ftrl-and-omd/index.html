<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=62670&amp;path=livereload" data-no-instant defer></script>
    <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" href="/favicon.ico" />

<title>Regret Analysis of FTRL and OMD Algorithms&nbsp;|&nbsp;Sihan Blog</title>
<meta
  name="title"
  content="Regret Analysis of FTRL and OMD Algorithms"
/>
<meta
  name="description"
  content="Introduction
In this note, we&rsquo;ll explore the regret analysis of both the Follow-The-Regularized-Leader (FTRL) algorithm and the Online Mirror Descent (OMD) algorithm. We&rsquo;ll highlight their similarities and differences, and demonstrate how, under certain conditions, they are essentially equivalent. This analysis includes detailed derivations and mathematical expressions.
Follow-The-Regularized-Leader (FTRL)
Problem Setup
Consider an online convex optimization problem over $T$ rounds. At each round $t$:

Decision Making: The learner selects $\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^n$.
Loss Revealing: An adversary reveals a convex loss function $f_t : \mathcal{X} \rightarrow \mathbb{R}$.
Loss Incurred: The learner incurs loss $f_t(\mathbf{x}_t)$.

Goal: Minimize the cumulative regret:"
/>
<meta
  name="keywords"
  content="online learning,optimization,"
/>

  <meta name="author" content="Sihan Wei" />




<meta property="og:url" content="http://localhost:62670/ftrl-and-omd/">
  <meta property="og:site_name" content="Sihan Blog">
  <meta property="og:title" content="Regret Analysis of FTRL and OMD Algorithms">
  <meta property="og:description" content="Introduction In this note, we’ll explore the regret analysis of both the Follow-The-Regularized-Leader (FTRL) algorithm and the Online Mirror Descent (OMD) algorithm. We’ll highlight their similarities and differences, and demonstrate how, under certain conditions, they are essentially equivalent. This analysis includes detailed derivations and mathematical expressions.
Follow-The-Regularized-Leader (FTRL) Problem Setup Consider an online convex optimization problem over $T$ rounds. At each round $t$:
Decision Making: The learner selects $\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^n$. Loss Revealing: An adversary reveals a convex loss function $f_t : \mathcal{X} \rightarrow \mathbb{R}$. Loss Incurred: The learner incurs loss $f_t(\mathbf{x}_t)$. Goal: Minimize the cumulative regret:">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2024-10-18T22:54:24-04:00">
    <meta property="article:modified_time" content="2024-10-18T22:54:24-04:00">
    <meta property="article:tag" content="online learning">
    <meta property="article:tag" content="optimization">





  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Regret Analysis of FTRL and OMD Algorithms">
  <meta name="twitter:description" content="Introduction In this note, we’ll explore the regret analysis of both the Follow-The-Regularized-Leader (FTRL) algorithm and the Online Mirror Descent (OMD) algorithm. We’ll highlight their similarities and differences, and demonstrate how, under certain conditions, they are essentially equivalent. This analysis includes detailed derivations and mathematical expressions.
Follow-The-Regularized-Leader (FTRL) Problem Setup Consider an online convex optimization problem over $T$ rounds. At each round $t$:
Decision Making: The learner selects $\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^n$. Loss Revealing: An adversary reveals a convex loss function $f_t : \mathcal{X} \rightarrow \mathbb{R}$. Loss Incurred: The learner incurs loss $f_t(\mathbf{x}_t)$. Goal: Minimize the cumulative regret:">





  <meta itemprop="name" content="Regret Analysis of FTRL and OMD Algorithms">
  <meta itemprop="description" content="Introduction In this note, we’ll explore the regret analysis of both the Follow-The-Regularized-Leader (FTRL) algorithm and the Online Mirror Descent (OMD) algorithm. We’ll highlight their similarities and differences, and demonstrate how, under certain conditions, they are essentially equivalent. This analysis includes detailed derivations and mathematical expressions.
Follow-The-Regularized-Leader (FTRL) Problem Setup Consider an online convex optimization problem over $T$ rounds. At each round $t$:
Decision Making: The learner selects $\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^n$. Loss Revealing: An adversary reveals a convex loss function $f_t : \mathcal{X} \rightarrow \mathbb{R}$. Loss Incurred: The learner incurs loss $f_t(\mathbf{x}_t)$. Goal: Minimize the cumulative regret:">
  <meta itemprop="datePublished" content="2024-10-18T22:54:24-04:00">
  <meta itemprop="dateModified" content="2024-10-18T22:54:24-04:00">
  <meta itemprop="wordCount" content="743">
  <meta itemprop="keywords" content="online learning,optimization">
<meta name="referrer" content="no-referrer-when-downgrade" />

    
    <link href="/simple.min.css" rel="stylesheet" />

    
    <link href="/style.min.css" rel="stylesheet" />

    

    

    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]                  
    }
  };
</script>
  
  </head>

  <body>
    <header>
      <nav>
  <a
    href="/"
    
    >Home</a
  >

  <a
    href="/blog/"
    
    >Blogs</a
  >

  <a
    href="/bbs/"
    
    >BBS</a
  >

  <a
    href="/about/"
    
    >About</a
  >


</nav>

<h1>Regret Analysis of FTRL and OMD Algorithms</h1>


    </header>
    <main>
      
  
    
      
      <p>
        <i>
          <time datetime="2024-10-18" pubdate>
            2024-10-18
          </time>
        </i>
      </p>
    
  
  
  <content>
    <h2 id="introduction">Introduction</h2>
<p>In this note, we&rsquo;ll explore the regret analysis of both the <strong>Follow-The-Regularized-Leader (FTRL)</strong> algorithm and the <strong>Online Mirror Descent (OMD)</strong> algorithm. We&rsquo;ll highlight their similarities and differences, and demonstrate how, under certain conditions, they are essentially equivalent. This analysis includes detailed derivations and mathematical expressions.</p>
<h2 id="follow-the-regularized-leader-ftrl">Follow-The-Regularized-Leader (FTRL)</h2>
<h3 id="problem-setup">Problem Setup</h3>
<p>Consider an online convex optimization problem over $T$ rounds. At each round $t$:</p>
<ol>
<li><strong>Decision Making</strong>: The learner selects $\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^n$.</li>
<li><strong>Loss Revealing</strong>: An adversary reveals a convex loss function $f_t : \mathcal{X} \rightarrow \mathbb{R}$.</li>
<li><strong>Loss Incurred</strong>: The learner incurs loss $f_t(\mathbf{x}_t)$.</li>
</ol>
<p><strong>Goal</strong>: Minimize the cumulative <strong>regret</strong>:</p>
$$
\text{Regret}_T = \sum_{t=1}^T f_{t}(\mathbf{x}_{t}) - \min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T f_{t}(\mathbf{x}).
$$<h3 id="ftrl-algorithm">FTRL Algorithm</h3>
<p>At each round $t$, the FTRL algorithm updates the decision by solving:</p>
$$
\mathbf{x}_t = \arg\min_{\mathbf{x} \in \mathcal{X}} \left\{ \eta \sum_{s=1}^{t-1} f_s(\mathbf{x}) + R(\mathbf{x}) \right\},
$$<p>where:</p>
<ul>
<li>$\eta > 0$ is the learning rate.</li>
<li>$R : \mathcal{X} \rightarrow \mathbb{R}$ is a strongly convex regularization function.</li>
</ul>
<h3 id="regret-analysis">Regret Analysis</h3>
<p><strong>Assumptions</strong></p>
<ol>
<li><strong>Convexity</strong>: Each loss function $f_t$ is convex.</li>
<li><strong>Lipschitz Continuity</strong>: The subgradients are bounded: $\| \nabla f_t(\mathbf{x}) \|_* \leq G$ for all $\mathbf{x} \in \mathcal{X}$.</li>
<li><strong>Strong Convexity</strong>: The regularizer $R$ is $\lambda$-strongly convex with respect to a norm $\| \cdot \|$.</li>
</ol>
<p><strong>Key Steps</strong></p>
<ol>
<li>
<p><strong>One-Step Regret Bound</strong></p>
<p>Using the convexity of $f_t$:</p>
$$
   f_t(\mathbf{x}_t) - f_t(\mathbf{x}^*) \leq \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}^* \rangle,
   $$<p>where $\mathbf{x}^* = \arg\min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T f_t(\mathbf{x})$.</p>
</li>
<li>
<p><strong>Regret Decomposition</strong></p>
<p>Summing over $t$:</p>
$$
   \text{Regret}_T \leq \sum_{t=1}^T \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}^* \rangle.
   $$</li>
<li>
<p><strong>Bounding the Inner Product</strong></p>
<p>Using the properties of the regularizer and the FTRL updates, we can relate the sum to the Bregman divergence $D_R$:</p>
$$
   \sum_{t=1}^T \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}^* \rangle \leq \frac{R(\mathbf{x}^*) - R(\mathbf{x}_1)}{\eta}.
   $$<p><strong>Bregman Divergence Definition</strong>:</p>
$$
   D_R(\mathbf{x}, \mathbf{y}) = R(\mathbf{x}) - R(\mathbf{y}) - \langle \nabla R(\mathbf{y}), \mathbf{x} - \mathbf{y} \rangle.
   $$</li>
<li>
<p><strong>Regret Bound</strong></p>
<p>Therefore, the total regret is bounded by:</p>
$$
   \text{Regret}_T \leq \frac{R(\mathbf{x}^*) - R(\mathbf{x}_1)}{\eta}.
   $$<p>By choosing $\eta$ appropriately (e.g., $\eta = \sqrt{\dfrac{2 [R(\mathbf{x}^*) - R(\mathbf{x}_1)]}{G^2 T}}$), we can achieve a regret bound of:</p>
$$
   \text{Regret}_T \leq G \sqrt{2 [R(\mathbf{x}^*) - R(\mathbf{x}_1)] T}.
   $$</li>
</ol>
<h2 id="online-mirror-descent-omd">Online Mirror Descent (OMD)</h2>
<h3 id="algorithm-steps">Algorithm Steps</h3>
<ol>
<li>
<p><strong>Initialization</strong>: Choose an initial point $\mathbf{x}_1 \in \mathcal{X}$.</p>
</li>
<li>
<p><strong>For each round $t = 1, \dots, T$</strong>:</p>
<p>a. <strong>Compute Subgradient</strong>:</p>
$$
   \mathbf{g}_t = \nabla f_t(\mathbf{x}_t).
   $$<p>b. <strong>Dual Space Update</strong>:</p>
$$
   \mathbf{z}_{t+1} = \mathbf{z}_t - \eta \mathbf{g}_t,
   $$<p>where $\mathbf{z}_t = \nabla \psi(\mathbf{x}_t)$.</p>
<p>c. <strong>Primal Space Update</strong>:</p>
$$
   \mathbf{x}_{t+1} = \nabla \psi^*(\mathbf{z}_{t+1}),
   $$<p>with $\psi^*$ being the convex conjugate of $\psi$.</p>
</li>
</ol>
<h3 id="regret-analysis-1">Regret Analysis</h3>
<p><strong>Assumptions</strong></p>
<ol>
<li><strong>Convexity</strong>: Each $f_t$ is convex.</li>
<li><strong>Lipschitz Continuity</strong>: Subgradients are bounded: $\| \mathbf{g}_t \|_* \leq G$.</li>
<li><strong>Strong Convexity</strong>: The mirror map $\psi$ is $\lambda$-strongly convex.</li>
</ol>
<p><strong>Key Steps</strong></p>
<ol>
<li>
<p><strong>Regret Decomposition</strong></p>
<p>The regret can be bounded by:</p>
$$
   \text{Regret}_T \leq \sum_{t=1}^T \langle \mathbf{g}_t, \mathbf{x}_t - \mathbf{x}^* \rangle.
   $$</li>
<li>
<p><strong>Using Mirror Descent Updates</strong></p>
<p>Utilizing the properties of the Bregman divergence $D_\psi$ and the mirror descent updates:</p>
$$
   \sum_{t=1}^T \langle \mathbf{g}_t, \mathbf{x}_t - \mathbf{x}^* \rangle = \frac{1}{\eta} \left[ D_\psi(\mathbf{x}^*, \mathbf{x}_1) - D_\psi(\mathbf{x}^*, \mathbf{x}_{T+1}) + \sum_{t=1}^T D_\psi(\mathbf{x}_{t+1}, \mathbf{x}_t) \right].
   $$</li>
<li>
<p><strong>Bounding the Bregman Divergences</strong></p>
<p>Since $D_\psi(\mathbf{x}^*, \mathbf{x}_{T+1}) \geq 0$ and $D_\psi(\mathbf{x}_{t+1}, \mathbf{x}_t) \leq \dfrac{\eta^2 G^2}{2 \lambda}$, we have:</p>
$$
   \text{Regret}_T \leq \frac{D_\psi(\mathbf{x}^*, \mathbf{x}_1)}{\eta} + \frac{\eta G^2 T}{2 \lambda}.
   $$</li>
<li>
<p><strong>Optimizing the Learning Rate</strong></p>
<p>Choosing:</p>
$$
   \eta = \sqrt{\dfrac{2 \lambda D_\psi(\mathbf{x}^*, \mathbf{x}_1)}{G^2 T}},
   $$<p>yields the regret bound:</p>
$$
   \text{Regret}_T \leq G \sqrt{\dfrac{2 D_\psi(\mathbf{x}^*, \mathbf{x}_1) T}{\lambda}}.
   $$</li>
</ol>
<h2 id="equivalence-of-ftrl-and-omd">Equivalence of FTRL and OMD</h2>
<p>Under certain conditions, FTRL and OMD are equivalent algorithms.</p>
<h3 id="conditions-for-equivalence">Conditions for Equivalence</h3>
<ul>
<li><strong>Matching Regularizers and Mirror Maps</strong>: If the regularizer $R$ in FTRL is identical to the mirror map $\psi$ in OMD.</li>
<li><strong>Unconstrained Domain</strong>: When the feasible set $\mathcal{X}$ is the entire space $\mathbb{R}^n$.</li>
</ul>
<h3 id="demonstration-of-equivalence">Demonstration of Equivalence</h3>
<ol>
<li>
<p><strong>FTRL Update in Terms of Gradients</strong></p>
<p>The FTRL update can be expressed as:</p>
$$
   \mathbf{x}_t = \arg\min_{\mathbf{x} \in \mathcal{X}} \left\{ \left\langle \eta \sum_{s=1}^{t-1} \mathbf{g}_s, \mathbf{x} \right\rangle + R(\mathbf{x}) \right\}.
   $$</li>
<li>
<p><strong>Relation to Dual Variables in OMD</strong></p>
<p>In OMD, the dual variable $\mathbf{z}_t$ is:</p>
$$
   \mathbf{z}_t = \nabla \psi(\mathbf{x}_t) = \mathbf{z}_1 - \eta \sum_{s=1}^{t-1} \mathbf{g}_s.
   $$</li>
<li>
<p><strong>Primal Update via Convex Conjugate</strong></p>
<p>The FTRL update becomes:</p>
$$
   \mathbf{x}_t = \nabla R^*\left( -\eta \sum_{s=1}^{t-1} \mathbf{g}_s \right),
   $$<p>which matches the OMD update when $R = \psi$:</p>
$$
   \mathbf{x}_t = \nabla \psi^*\left( \nabla \psi(\mathbf{x}_1) - \eta \sum_{s=1}^{t-1} \mathbf{g}_s \right).
   $$</li>
</ol>
<h3 id="conclusion">Conclusion</h3>
<p>By aligning the regularization function in FTRL with the mirror map in OMD and considering the unconstrained domain, the updates of both algorithms coincide. This demonstrates that FTRL and OMD are essentially equivalent under these conditions, offering different perspectives on the same optimization process.</p>

  </content>
  <p>
    
      <a href="/blog/online-learning/">#online learning</a>&nbsp;&nbsp;
    
      <a href="/blog/optimization/">#optimization</a>&nbsp;&nbsp;
    
  </p>

    </main>
    <footer>
      
  <span>© 2024 Sihan Wei</span>


  <span>
    |
    Made with
    <a href="https://github.com/maolonglong/hugo-simple/">Hugo ʕ•ᴥ•ʔ Simple</a>
  </span>


    </footer>

    
</body>
</html>
