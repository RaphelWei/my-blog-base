<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Random on Sihan Blog</title>
    <link>http://localhost:1313/tags/random/</link>
    <description>Recent content in Random on Sihan Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Dec 2021 18:37:42 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/random/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2021年度总结</title>
      <link>http://localhost:1313/post/2021/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 29 Dec 2021 18:37:42 -0500</pubDate>
      <guid>http://localhost:1313/post/2021/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/</guid>
      <description>&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;&#xA;&lt;p&gt;2021年就快要过去了，我很怀念它。&lt;/p&gt;&#xA;&lt;p&gt;其实我本来是不太爱写年度总结这种东西的，一方面是因为懒，另一方面也是因为懒。&lt;/p&gt;&#xA;&lt;p&gt;不过今年总归是有点特殊的，毕竟经历了不少人和事。从旧的学校毕业了，在新的学校开始了新的旅程。于是最后决定写点什么纪念一下。&lt;/p&gt;&#xA;&lt;p&gt;说是年度总结，其实更像是个申请总结。时间跨度大概是从去年暑假到今年开学。&lt;/p&gt;&#xA;&lt;h2 id=&#34;八月&#34;&gt;八月&lt;/h2&gt;&#xA;&lt;p&gt;我的Thesis，或者说Capstone Project更合理一点，是实现Nesterov那篇关于Tensor Methods的算法。简单来说，常用的gradient descent是基于泰勒的一阶展开&#xA;$$&#xA;f(y)\approx f(x)+\langle \nabla f(x),y-x\rangle&#xA;$$&#xA;然而大家都知道first-order method收敛很慢，即使是在strongly convex的条件下也只有$O\left(\log(1/\epsilon)\right)$。Nesterov提出了一种利用$p$ 阶$(p\geq2)$泰勒展开来求解优化问题的算法&#xA;$$&#xA;\arg\min_{y\in\mathbb{R}^n}\sum_{r=0}^{p} \frac{1}{r !} \nabla^{r} f(x) [y-x, \ldots, y-x]+\frac{M_p}{(p+1) !}|y-x|_{2}^{p+1}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;这个算法的优点是快，在&lt;code&gt;条件&lt;/code&gt;拉满的情况下可以达到$$O\left((1/\epsilon)^\frac{M_p}{5p+2}\right)$$&lt;/p&gt;&#xA;&lt;p&gt;&lt;!-- raw HTML omitted --&gt;这个算法的缺点是，&lt;code&gt;条件&lt;/code&gt;过于严格，又要convex又要Lipschitz。没人会傻乎乎地用这玩意训练神经网络。&lt;!-- raw HTML omitted --&gt;这段划掉。&lt;/p&gt;&#xA;&lt;p&gt;这个东西写了大概两个月，暑假结束的时候跟老板简单聊了聊。老板建议我把代码整理一下，可以搞个package。然后设计一些实验，看看能不能投出去。&lt;/p&gt;&#xA;&lt;p&gt;我最后拒绝了这个提议，因为我觉得这玩意实在没啥用。&lt;/p&gt;&#xA;&lt;p&gt;八月底的时候正式把申请提上了议程。我简单地做了个个人主页，&lt;strong&gt;优点是发套瓷邮件的时候不需要在附件里放CV，直接在邮件正文里附上主页链接就好了。因为很多老师不太喜欢下载附件。另一个好处是可以在网页里加上tracker，这样在Google Analytics里根据主页访问记录的地址和时间就可以判断哪些老师浏览了我的主页，从而继续跟进。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;因为疫情，当时好多学校都hiring freeze了（虽然这个是针对AP招聘的，不过侧面也反映出学校没钱）。当时心里很慌，因为感觉疫情加持下的申请会异常激烈。有一天逛一亩三分地的时候，看到一位Georgia Tech的AP在找RA，而且他本人还是UMN的校友，就很高兴地给他发了封邮件。他很快就回复了，说是约个时间面试。&lt;/p&gt;&#xA;&lt;p&gt;面试很流水线化，我简单做了个slides讲了一下就结束了。这个老师之前是ME系的PhD，现在Aerospace Engineering (就是搞火箭的那个系)当AP。之前他一直在做控制理论和优化，不知道为什么突然转行搞起ML theory了。他想让我做nonconvex-nonconcave minimax optimization，相当于是给我挖了个新坑。因为GAN和adversarial training很火，催生了这么个研究它们背后理论的领域。我想了一下，好像可以从optimizer的角度搞点东西，现在主流的训练还在用Adam或者SGD，说不定可以把我之前做的破烂往里面塞一塞。我最后搞了个second-order(再高阶的用来训练神经网络想着都玄幻)的方法，在MNIST跑了下效果还行，不过ImageNet还是算了。这个坑填到这我就放弃了，别问，问就是拿到offer了，谁还给他免费打工啊。&lt;/p&gt;&#xA;&lt;h2 id=&#34;九月&#34;&gt;九月&lt;/h2&gt;&#xA;&lt;p&gt;新学期开学的时候，系里面组织了一个类似于新老生见面会的活动。新生会填一个问卷，回答诸如research interest，以及对哪个lab或者professor感兴趣的问题。系里面会统一收集起来，根据新生的兴趣给他们匹配lab。然后每一个lab出一个老生作为代表 (Lab Ambassador)，回答新生的问题。&lt;/p&gt;&#xA;&lt;p&gt;我当时为了老板能在推荐信里给我多写几句好话，便主动请缨。完事之后，系里面还让我们把Twitter个人主页好好弄一下，然后带个UMN CS的tag发推自我介绍，这样新老生相互之间可以很快认识（不得不说美国人真的很热衷于connection这一套）。我闲得无聊把自己几百年不用的Twitter也找出来稍微搞了一下，没想到这个后来居然给我带来了不少机会，下面会谈到。&lt;/p&gt;&#xA;&lt;p&gt;这学期还申请到了TA，不过是在商学院，给一群Business Analytics专业的学生教Python。当TA的经历也蛮有意思的，给这些学生讲课让我想起了自己当年刚学写代码的时候也是啥都不懂，总是问一堆很天真的问题，虽然现在代码写的依然很烂。&lt;/p&gt;&#xA;&lt;p&gt;第一次Office hour的时候，一个黑人小哥发邮件问我，为什么按照课上示例的代码运行会报错。我看了下他发的截图，检查之后发现代码没有问题，就给他开了个Zoom视频，发现他用Spyder（就是下载anaconda自带的那玩意儿）一行一行地在运行代码。于是运行到&lt;code&gt;if&lt;/code&gt;语句那一行的时候，编译器说你这个代码不完整啊，就给报错了。我就给他讲，你点这个绿三角运行整个文件就好了。小哥对我很是感激，然后就盯上我了，可劲给我发邮件问问题，有一次直接说&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;I&amp;rsquo;m writing to ask if we can meet physically&amp;hellip;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
