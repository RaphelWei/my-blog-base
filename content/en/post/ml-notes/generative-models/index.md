---
author : "Sihan Wei"
title: "Generative Models: The Past and Present"
date: 2025-04-18T17:41:37-04:00
tags : ["Machine Learning", "Generative Models", "Diffusion Models", "Transformers", "GANs", "LLMs", "Long read"]
categories : ["ml-notes"]
description : "From probabilistic roots to the age of Transformers."
image: ""
draft: true
---

## Introduction
It was 10 years ago when I first heard about the term "Generative Models". Back then, Generative Adversarial Networks (GANs) almost dominated the top venues of ML community. People came up with different GAN variants. However, 10 years later, the landscape of generative models has changed dramatically. The rise of diffusion models and the success of large language models (LLMs) have reshaped our understanding of generative modeling. Nowadays, if you try to land a job in ML, it's almost GenAI or the others. 

Large language models (LLMs) is now a topic that everyone in this field is talking about. But before we are really diving into it, let's take the time machine back to decades ago and see how generative models have evolved over the years.

## The Early Days: Probabilistic Models

## The Graphical Era: Bayesian Networks and HMMs

## Approximating the Intractable: Variational Methods


## The GAN Revolution

## Autogreressive Models: Pixel by pixel

(talk about PixelCNN/PixelRNN, and early language models like RNNs and LSTMs)

## From Noise to Masterpieces: Diffusion Models



## Scaling Up: Transformers and the GenAI Era


## Conclusion